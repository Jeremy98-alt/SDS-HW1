---
title: "Part A"
author: "Jeremy Sapienza & Stefano D'Arrigo"
date: "22/11/2020"
output: html_document
---

<style>
body {
text-align: justify}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **SDS HOMEWORK** 
### **PART A**

**Randomize this...**

$~$

We have delved into the world of graphs, but what is a graph? It is a network of vertices with directed or undirected edges defined by **G=<V,E>**. In this first part of the homework, we deal with a graph having V = {1, 2, 3, 4, 5, 6, 7} and E = {{1, 2}, {1, 4}, {2, 3}, {4, 3}, {3, 5}, {5, 6}, {5, 7}}.

For this reason, we consider a library provided by R: **igraph**. We recall this library and we create the graph for this exercise.

```{r include=FALSE}
#pick a specific (small) graph G
library(igraph)
```

$~$

```{r}
G <- graph( edges=c(1,2, 2,3, 1,4, 4,3, 3,5, 5,6, 5,7),  n=7, directed=F)
plot(G)
```

Now let's face with the **Max-Cut** problem, which askes us to solve:

$~$

<center> max{ card(*$\delta$*(*U*)) for *U* $\subseteq$ *V* } </center>

$~$

To calculate the max cut of the main graph G, we use another library of R called **sdpt3r**; then we create the matrix with few methods provided by the igraph library

$~$

```{r include=FALSE}
#run the library and obtain the max-cut(G)
library(sdpt3r)
```

```{r}
#firstly search to create our adjacency_matrix from G and convert it to a matrix
adj <- as.matrix(as_adjacency_matrix(G))

#maximum cut, use the sdpt3r method
result <- maxcut(adj)

#show the maxcut; we take the absolute value of the maxcut result, because the main formula gives a negative interpretation
maxcut <- abs(result$pobj)
maxcut

```


$~$

Now.. What is U? It is a subset of V, our initial graph. U is a subgraph chosen with the Bernoulli distribution, as we will discuss later on.
For each vertex, set U $\subseteq$ V and define the cut determined by U as: 

$~$

<center> $\delta$(*U*) = { {u,v} $\in$ *E* such that u $\in$ *U* and v $\notin$ *U* }</center>

$~$

So, let OPT = OPT(*G*) be the size of the maximum cut we are chasing. Our goal is to find an algorithm for which there is a
factor $\alpha$ > 0 independent of the graph *G* such that the set *U* it builds is guaranteed to have:

$~$

<center>  card(*$\delta$*(*U*)) $\geq$ $\alpha$ x OPT </center>

$~$

By the way, we are asked about U subset of V. This is created every time we run the **Randomized Max-Cut Algorithm**. The algorithm says "create U as *random* subset of V; that is, for each vertex v $\in$ V, flip a coin: if Heads, add v to U otherwise do not."

$~$

Hence, we have to consider a **Bernoulli distribution** to pick the vertices of G to be inserted into the subset U; thus, a vertex is picked with probability p = 1/2. In particular, a vertex is inserted into U if the outcome of the coin is equal to Head (1: success).

$~$

<center> $p_x$(x) = $p^x(1-p)^{(1-x)}$, x $\in$ {0, 1} </center>

$~$

Flipping the coin n times, we obtain a **Binomial distribution** as follows:

$~$

```{r}
#Binomial distribution, because we are taking this experiment M times
colo <- c(rgb(32/255, 74/255, 135/255, 0.7))
          
#Set parameters
pp <- 0.5
nn <- 100

#Plot PMF
plot(0:nn, dbinom(0:nn, nn, pp),
     xlim = c(0,100), ylim = c(0,0.10),
     type="b", lty=3,
     xlab= "", ylab ="",
     col=gray(.8), pch=21, bg=colo)

grid()

legend("topleft", c("p = 0.5, n = 100"),
       col = colo, pch=19, bty="n", cex = .8)
```


$~$

The following piece of code implements the steps described above; we import the library **Rlab** to leverage the built-in function `rbern`:

$~$

```{r include=FALSE}
#first install and use the package that contains the bernoulli distro 
library(Rlab)
```


```{r}
#create the function for our goal to obtain our subset of V
subset.V <- function(vertex.names, number_of_vertex){
  U <- c()
  
  #according to the bernoulli distribution, if the result is equal to one we append the i-th vertex into our subset U
  for(i in 1:number_of_vertex){
    randBern <- rbern(1, 0.5)
    if(randBern == 1)
      U <- append(U, vertex.names[i])
  }
  
  return(U)
}

# get the vertex's names, in this case this function as_ids() create a vector of vertex ... or do as.character(V(G))
vertex.names <- as_ids(V(G))

#try M times to generate the subset of V
U <- subset.V(vertex.names, gsize(G))
U

```

$~$

The goal of the homework is to run Randomized Max-Cut Algorithm for a large number M of times and to evaluate the average cut-size over these M simulations, comparing it with the theoretical bound opt(G)/2.

Before starting to run the algorithm M times, we set the adjacency matrix for one side and obtain the edges of our G graph to create the $\delta$(*U*) set.

$~$

```{r}
#set zero the Lower Triangular Part of a Matrix to avoid catching the duplicates edges...the graph is undirected!
adj[lower.tri(adj, diag=FALSE)] <- 0 

#create the list of edges from our G graph
list.edges <- which(adj==1, arr.ind = TRUE) #save the true link
colnames(list.edges) <- c("V1", "V2") #change columns name..

list.edges
```

$~$

After the creation of the dataframe that contains the edges of G graph, by using the following functions we evaluate the average cut-size over these M simulations and compare it with the theoretical bound OPT(G)/2.

$~$

```{r}
#create subset U edges for the maxcut..
num.edges.U <- function(U, list.edges){
  edges <- c()
  
  #according to the rule of delta(U)
  `%notin%` <- Negate(`%in%`) #define new operator
  for(row in 1:nrow(list.edges)){
    if( (list.edges[row, 1] %in% U) && (list.edges[row, 2] %notin% U) ){
      edges <- append(edges, c(list.edges[row, 1], list.edges[row, 2]) )
    }
  }

  return(length(edges))
}

#calculate M times and return the average cut size
averageCutSize <- function(vertex.names, list.edges, M){
  avg.cutsize <- c()
  
  for(i in 1:M){
    U <- subset.V(vertex.names, length(vertex.names)) #define the subset of V
    card.U <- num.edges.U(U, list.edges) #cardinality of U
    
    #append the result
    avg.cutsize <- append(avg.cutsize, card.U)
  }
  
  #finally return the mean!
  return(mean(avg.cutsize))
}
```

$~$

We call the averageCutSize() for a large number of times to see if the real performance is compliant to the expected performance:

$~$

```{r}
#call M times the averageCutSize
averageCutSize(vertex.names, list.edges, 1000)
```

$~$

We compare our empirical result with the theoretical bound, which is:

$~$

<center> $\mathbb{E}$(card(*$\delta$*(*U*))) $\geq$  OPT/2 </center>

$~$

```{r}
theoretical.bound <- maxcut/2
theoretical.bound
```

$~$

The result we got is almost compliant to the theoretical bound, even though the previous inequality is not verified with the small graph we have considered so far. At this point, we hypothesize that testing the algorithm on a graph with a greater number of vertices would have a better performance, fitting the theoretical bound.

$~$

Thus, we try with a random graph, according to the Erdős and Rényi model. We choose $\frac{1}{3}$ as the probability of drawing an edge between two nodes:

$~$

```{r}
# Change the graph size to see if there is an impact on the performance
G2 <- erdos.renyi.game(40, 1/3)
plot(G2)
```

$~$

In the case of a graph of 40 nodes, the max-cut is:

$~$

```{r}
#firstly search to create our adjacency_matrix from G and convert it to a matrix
adj2 <- as.matrix(as_adjacency_matrix(G2))

#maximum cut
result2 <- maxcut(adj2)

#return the maximum cut, use the value returned
maxcut2 <- abs(result2$pobj)
maxcut2
```

$~$

Running the steps we previously followed in the case of the smaller graph, we obtain the subset U:

$~$

```{r}
#show the result
vertex.names <- as_ids(V(G2))
U <- subset.V(vertex.names, gsize(G2))
U
```

$~$

Again, we fill the adjacency matrix in a convenient manner:

$~$

```{r}
adj2[lower.tri(adj2, diag=FALSE)] <- 0 

list.edges <- which(adj2==1, arr.ind = TRUE)
colnames(list.edges) <- c("V1", "V2")
```

$~$

In this case, the max-cut bound is:

$~$

```{r}
theoretical.bound2 <- maxcut2/2
theoretical.bound2
```

$~$

Finally, we calculate the cut-size for M=1000 times and we get the average cut-size: 

$~$

```{r}
averageCutSize(vertex.names, list.edges, 1000)
```
 
$~$

The result shows that, with a graph of 40 nodes, the average cut-size overcomes the theoretical bound.
