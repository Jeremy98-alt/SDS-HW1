---
title: "Part A"
author: "Jeremy Sapienza & Stefano D'Arrigo"
date: "22/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# **SDS HOMEWORK** 
### **PART A**

**Randomize this...**

$~$

We are delve in the world of graphs, but what is a graph? Is a network of verteces with their edges directed or undirected defined by **G=<V,E>**. In this first part of the homework we are encountering a graph with V = {1, 2, 3, 4, 5, 6, 7} and E = {{1, 2}, {1, 4}, {2, 3}, {4, 3}, {3, 5}, {5, 6}, {5, 7}}

For this reason, we are focussing in a library provided by R **igraph**, we recall this library and we create the graph for this exercise.

```{r include=FALSE}
#pick a specific (small) graph G
library(igraph)
```

$~$

```{r}
G <- graph( edges=c(1,2, 2,3, 1,4, 4,3, 3,5, 5,6, 5,7),  n=7, directed=F)
plot(G)
```

Now let's face with the **Max-Cut** problem, this problem says that we must solve:

$~$

<center> max{ card(*$\delta$*(*U*)) for *U* $\subseteq$ *V* } </center>

$~$

To calculate the max cut of the main graph G, we used another library of R called **sdpt3r**, then we created the matrix with few methods provided by the igraph library

$~$

```{r include=FALSE}
#run the library and obtain the max-cut(G)
library(sdpt3r)
```

```{r}
#firstly search to create our adjacency_matrix from G and convert it to a matrix
adj <- as.matrix(as_adjacency_matrix(G))

#maximum cut, use the sdpt3r method
result <- maxcut(adj)

#show the maxcut, we made the absolute value of the maxcut result, because the main formula gives a negative interpretation
maxcut <- abs(result$pobj)
maxcut

```


$~$

Now.. What is U? Is a subset of V, our initial graph. U is a subgraph chose with the Bernoulli distribution, but then we will delve in this argument.
For any vertex set U $\subseteq$ V, define the cut determined by U as: 

$~$

<center> $\delta$(*U*) = { {u,v} $\in$ *E* such that u $\in$ *U* and v $\notin$ *U* }</center>

$~$

So, let OPT = OPT(*G*) be the size of the maximum cut we are chasing. Our goal is to find an algorithm for which there is a
factor $\alpha$ > 0 independent on the graph *G* such that the set *U* it builds is guaranteed to have:

$~$

<center>  card(*$\delta$*(*U*)) $\geq$ $\alpha$ x OPT </center>

$~$

By the way, we are spoke about U the subset of V. This is create every time we run the **Randomized Max-Cut Algorithm**. the algorithm says "create U as *random* subset of V; that is, for each vertex v $\in$ V, flip a coin: if Heads, add v to U otherwise do not."

$~$

We see in this last phrase that is useful to use the **Bernoulli distribution** to obtain each time a success or a failure that allow us to insert in our subset U a vertex of G, with a probability p = 1/2. Because this event simulates the flip of a coin with the results are based on head (1: success) or tail (0: failure), in our case we want to consider the event equal to Head (1: success). 

$~$

<center> $p_x$(x) = $p^x(1-p)^{(1-x)}$, x $\in$ {0, 1} </center>

$~$

```{r}
#Binomial distribution, because we are taking this experiment M times
colo <- c(rgb(32/255, 74/255, 135/255, 0.7))
          
#Set parameters
pp <- 0.5
nn <- 100

#Plot PMF
plot(0:nn, dbinom(0:nn, nn, pp),
     xlim = c(0,100), ylim = c(0,0.10),
     type="b", lty=3,
     xlab= "", ylab ="",
     col=gray(.8), pch=21, bg=colo)

grid()

legend("topleft", c("p = 0.5, n = 100"),
       col = colo, pch=19, bty="n", cex = .8)
```


$~$

In this way we created the piece of code about the creation of this subset, we used another library called **Rlab** useful to achieve our goal:

$~$

```{r include=FALSE}
#first install and use the package that contains the bernoulli distro 
library(Rlab)
```


```{r}
#create the function for our goal to obtain our subset of V
subset.V <- function(vertex.names, number_of_vertex){
  U <- c()
  
  for(i in 1:number_of_vertex){
    randBern <- rbern(1, 0.5)
    if(randBern == 1)
      U <- append(U, vertex.names[i])
  }
  
  return(U)
}

# get the vertex's names, in this case this function as_ids() create a vector of vertex ... or do as.character(V(G))
vertex.names <- as_ids(V(G))

#try M times to generate the subset of V
U <- subset.V(vertex.names, gsize(G))
U

```

$~$

The homework pretends to run the Randomized Max-Cut Algorithm a large number (M) of times and evaluate the average cut-size over these M simulations comparing it with the theoretical bound opt(G)/2 to follow the performance analysis.

Before starting to run M times the algorithm we set the adjacency matrix for one side and obtain the edges of our G graph to create the $\delta$(*U*) set.

$~$

```{r}
#set zero the Lower Triangular Part of a Matrix to avoid catching the duplicates edges...the graph is undirected!
adj[lower.tri(adj, diag=FALSE)] <- 0 

#create the list of edges from our G graph
list.edges <- which(adj==1, arr.ind = TRUE) #save the true link
colnames(list.edges) <- c("V1", "V2") #change columns name..

list.edges
```

$~$

After the creation of this dataframe that contains the edges of G graph, we made this algorithm on according to the initial request "Evaluate the average cut-size over these M simulations and compare it with the theoretical bound opt(G)/2."

$~$

```{r}
#create subset U edges for the maxcut..
num.edges.U <- function(U, list.edges){
  edges <- c()
  
  `%notin%` <- Negate(`%in%`) #define new operator
  for(row in 1:nrow(list.edges)){
    if( (list.edges[row, 1] %in% U) && (list.edges[row, 2] %notin% U) ){
      edges <- append(edges, c(list.edges[row, 1], list.edges[row, 2]) )
    }
  }

  return(length(edges))
}

#calculate M times and return the average cut size
averageCutSize <- function(vertex.names, list.edges, M){
  avg.cutsize <- c()
  
  for(i in 1:M){
    U <- subset.V(vertex.names, length(vertex.names)) #define the subset of V
    card.U <- num.edges.U(U, list.edges) #cardinality of U
    
    #append the result
    avg.cutsize <- append(avg.cutsize, card.U)
  }
  
  #finally return the mean!
  return(mean(avg.cutsize))
}
```

$~$

Call a large number of times the averageCutSize() to see if the performance is respected

$~$

```{r}
#call M times the averageCutSize
averageCutSize(vertex.names, list.edges, 1000)
```

$~$

Now, we can see.. if the performance analysis is respected, it says "Let U be the set chosen by this (randomized) algorithm.
Then the expected size of the cut set determined by U is at least OPT/2 :"

$~$

<center> $\mathbb{E}$(card(*$\delta$*(*U*))) $\geq$  OPT/2 </center>

$~$

```{r}
theoretical.bound <- maxcut/2
theoretical.bound
```

$~$

The performance is respected, but... If we increase the size of our G graph (maybe with a random graph) which is the result?

$~$

Now we prefer creating a random graph with the erdos renyi model. We have in this case our model based on the probability to link two nodes with a probability p, we set p = 1/2 for any reason in particular.

$~$

```{r}
# Change the graph size to see if there is an impact on the performance
G2 <- erdos.renyi.game(20, 1/3)
plot(G2)
```

$~$

Now we made the same operations that we saw previously:

$~$

```{r}
#firstly search to create our adjacency_matrix from G and convert it to a matrix
adj2 <- as.matrix(as_adjacency_matrix(G2))

#maximum cut
result2 <- maxcut(adj2)

#return the maximum cut, use the value returned
maxcut2 <- abs(result2$pobj)
maxcut2
```

$~$

This is the value, now we passed to generate our subset based on bernoulli distribution

$~$

```{r}
#show the result
vertex.names <- as_ids(V(G2))
U <- subset.V(vertex.names, gsize(G2))
U
```

$~$

like in the previous example with less verteces, we selected the edges and we saved them to our operations

$~$

```{r}
adj2[lower.tri(adj2, diag=FALSE)] <- 0 

list.edges <- which(adj2==1, arr.ind = TRUE)
colnames(list.edges) <- c("V1", "V2")
```

$~$

We calculated like the previous steps the maxcut for this graph

$~$

```{r}
theoretical.bound2 <- maxcut2/2
theoretical.bound2
```

$~$

This is our theoretical bound for this our huge graph and we can see now M times the averageCutSize doing the operation M times

$~$

```{r}
averageCutSize(vertex.names, list.edges, 100)
```
 
$~$

We can see that the value is pretty high and the performance is still respected

